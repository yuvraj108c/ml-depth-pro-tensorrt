<div align="center">

# ML-Depth-Pro TensorRT ‚ö°

[![python](https://img.shields.io/badge/python-3.10.12-green)](https://www.python.org/downloads/release/python-31012/)
[![cuda](https://img.shields.io/badge/cuda-12.4-green)](https://developer.nvidia.com/cuda-downloads)
[![trt](https://img.shields.io/badge/TRT-10.5.0-green)](https://developer.nvidia.com/tensorrt)

</div>

This project provides a [TensorRT](https://github.com/NVIDIA/TensorRT) implementation of [ml-depth-pro](https://github.com/apple/ml-depth-pro) by Apple, enabling 5-6x faster inference speeds.

If you like the project, please give me a star! ‚≠ê

---

## ‚è±Ô∏è Performance

_Note: Inference was done in FP16, with a warm-up period of 5 frames. The reported time corresponds to the last inference._
| Device | Model Resolution| Inference Time (ms) |
| :----: | :-: | :-: |
|  H100  | 1536 x 1536  | 63 |

## üöÄ Installation

```bash
git clone https://github.com/apple/ml-depth-pro
cd ./ml-depth-pro
pip install -e .
bash get_pretrained_models.sh

git clone https://github.com/yuvraj108c/ml-depth-pro-tensorrt
mv ./ml-depth-pro-tensorrt/* . && rm -r ./ml-depth-pro-tensorrt
pip install -r requirements.txt
```

## üõ†Ô∏è Building onnx 
```bash
python export_onnx.py 
```

## üõ†Ô∏è Building tensorrt 
```bash
python export_trt.py 
```

## ‚ö° Inference on single image
```bash
python infer_trt.py 
```

## üìì Notes

The model currently supports images with a fixed resolution of 1536 x 1536  

## ü§ñ Environment tested

Ubuntu 22.04 LTS, Cuda 12.4, Tensorrt 10.5.0, Python 3.10, H100 GPU
